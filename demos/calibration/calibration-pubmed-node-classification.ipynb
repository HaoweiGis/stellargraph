{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Attribute Inference (multi-class) using GraphSAGE and the Pubmed-Diabetes citation network with calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates probability calibration for multi-class node attribute inference. The classifier used is GraphSAGE and the dataset is the citation network Pubmed-Diabetes. Our task is to predict the subject of a paper (the nodes in the graph) that is one of 3 classes. The data are the network structure and for each paper a 500-dimensional TF/IDF word vector.\n",
    "\n",
    "The notebook demonstrates the use of `stellargraph`'s `TemperatureCalibration` and `IsotonicCalibration` classes as well as supporting methods for calculating the Expected Calibration Error (ECE) and plotting reliability diagrams.\n",
    "\n",
    "Since the focus of this notebook is to demonstrate the calibration of `stellargraph`'s graph neural network models for classification, we do not go into detail on the training and evaluation of said models. We suggest the reader considers the following notebook for more details on how to train and evaluate a GraphSAGE model for node attribute inference,\n",
    "\n",
    "[Stellargraph example: GraphSAGE on the CORA citation network](https://github.com/stellargraph/stellargraph/blob/master/demos/node-classification-graphsage/graphsage-cora-node-classification-example.ipynb)\n",
    "\n",
    "**References**\n",
    "1. Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec arXiv:1706.02216 \n",
    "[cs.SI], 2017. ([link](http://snap.stanford.edu/graphsage/))\n",
    "\n",
    "2. On Calibration of Modern Neural Networks. C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. \n",
    "ICML 2017. ([link](https://geoffpleiss.com/nn_calibration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from stellargraph import TemperatureCalibration, IsotonicCalibration\n",
    "from stellargraph import plot_reliability_diagram, expected_calibration_error\n",
    "\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a GraphSAGE model, a node generator, and the number of predictions per point\n",
    "# this method makes n_predictions number of predictions and then returns the average\n",
    "# prediction for each query node.\n",
    "def predict(model, node_generator, n_predictions=1):\n",
    "    preds = []\n",
    "    for i in range(n_predictions):\n",
    "        preds.append(model.predict_generator(node_generator))\n",
    "    preds_ar = np.array(preds)\n",
    "    print(preds_ar.shape)\n",
    "    return np.mean(preds_ar, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20  # Numper of training epochs for GraphSAGE model.\n",
    "n_predictions = 5  # number of predictions per query node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pubmed-Diabetes network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The PubMed Diabetes dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMed Diabetes dataset is already downloaded\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.PubMedDiabetes()\n",
    "display(HTML(dataset.description))\n",
    "dataset.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the data so that we can create a `networkx` object that can be used by `stellargraph`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the graph from edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = pd.read_csv(os.path.join(dataset.data_directory, 'Pubmed-Diabetes.DIRECTED.cites.tab'), \n",
    "                         sep=\"\\t\", skiprows=2, \n",
    "                         header=None )\n",
    "edgelist.drop(columns=[0,2], inplace=True)\n",
    "edgelist.columns = ['source', 'target']\n",
    "# delete unneccessary prefix \n",
    "edgelist['source'] = edgelist['source'].map(lambda x: x.lstrip('paper:')) \n",
    "edgelist['target'] = edgelist['target'].map(lambda x: x.lstrip('paper:'))\n",
    "edgelist[\"label\"] = \"cites\"  # set the edge type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19127292</td>\n",
       "      <td>17363749</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19668377</td>\n",
       "      <td>17293876</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1313726</td>\n",
       "      <td>3002783</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19110882</td>\n",
       "      <td>14578298</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18606979</td>\n",
       "      <td>10333910</td>\n",
       "      <td>cites</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source    target  label\n",
       "0  19127292  17363749  cites\n",
       "1  19668377  17293876  cites\n",
       "2   1313726   3002783  cites\n",
       "3  19110882  14578298  cites\n",
       "4  18606979  10333910  cites"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features and subject for the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_as_dict = []\n",
    "with open(os.path.join(dataset.data_directory, \"Pubmed-Diabetes.NODE.paper.tab\")) as fp:\n",
    "    for line in itertools.islice(fp, 2, None):\n",
    "        line_res = line.split(\"\\t\")\n",
    "        pid = line_res[0]\n",
    "        feat_name = ['pid'] + [l.split(\"=\")[0] for l in line_res[1:]][:-1] # delete summary\n",
    "        feat_value = [l.split(\"=\")[1] for l in line_res[1:]][:-1] # delete summary\n",
    "        feat_value = [pid] + [ float(x) for x in feat_value ] # change to numeric from str\n",
    "        row = dict(zip(feat_name, feat_value))\n",
    "        nodes_as_dict.append(row)\n",
    "        \n",
    "# Create a Pandas dataframe holding the node data        \n",
    "node_data = pd.DataFrame(nodes_as_dict)\n",
    "node_data.fillna(0, inplace=True)\n",
    "node_data['label'] = node_data['label'].astype(int)\n",
    "node_data['label'] = node_data['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(node_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data['pid'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data['pid'] = node_data.pid.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.index = node_data['pid']\n",
    "node_data.drop(columns=['pid'], inplace=True)\n",
    "node_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning we want to take a subset of the nodes for training, and use the rest for testing. We'll use scikit-learn again to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(node_data, \n",
    "                                                         train_size=0.75, \n",
    "                                                         test_size=None, \n",
    "                                                         stratify=node_data['label'])\n",
    "train_data, val_data = model_selection.train_test_split(train_data, \n",
    "                                                        train_size=0.75, \n",
    "                                                        test_size=None, \n",
    "                                                        stratify=train_data['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note using stratified sampling gives the following counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(train_data['label']), Counter(val_data['label']), Counter(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set has class imbalance that might need to be compensated, e.g., via using a weighted cross-entropy loss in model training, with class weights inversely proportional to class support. However, we will ignore the class imbalance in this example, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to numeric arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our categorical target, we will use one-hot vectors that will be fed into a soft-max Keras layer during training. To do this conversion ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = feature_extraction.DictVectorizer(sparse=False)\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_data[[\"label\"]].to_dict('records'))\n",
    "val_targets = target_encoding.fit_transform(val_data[[\"label\"]].to_dict('records'))\n",
    "test_targets = target_encoding.transform(test_data[[\"label\"]].to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same for the node attributes we want to use to predict the subject. These are the feature vectors that the Keras model will use as input. The dataset contains attributes 'w-*' that correspond to words found in that publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = node_data.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the GraphSAGE model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a StellarGraph object from the NetworkX graph and the node features and targets. It is StellarGraph objects that we use in this library to perform machine learning tasks on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarGraph(Gnx, node_features=node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed data from the graph to the Keras model we need a node generator. The node generators are specialized to the model and the learning task so we choose the `GraphSAGENodeMapper` as we are predicting node attributes with a GraphSAGE model.\n",
    "\n",
    "We need two other parameters, the `batch_size` to use for training and the number of nodes to sample at each level of the model. Here we choose a two-level model with 10 nodes sampled in the first layer, and 5 in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50; num_samples = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `GraphSAGENodeGenerator` object is required to send the node features in sampled subgraphs to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GraphSAGENodeGenerator(G, batch_size, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we map only the training nodes returned from our splitter and the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_data.index, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify our machine learning model, we need a few more parameters for this:\n",
    "\n",
    " * the `layer_sizes` is a list of hidden feature sizes of each layer in the model. In this example we use 32-dimensional hidden node features at each layer.\n",
    " * The `bias` and `dropout` are internal parameters of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32],\n",
    "    generator=generator,\n",
    "    bias=True,\n",
    "    dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model to predict the 3 categories using Keras softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp, x_out = graphsage_model.build()\n",
    "logits = layers.Dense(units=train_targets.shape[1], activation=\"linear\")(x_out)\n",
    "\n",
    "prediction = layers.Activation(activation=\"softmax\")(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the actual Keras model with the graph inputs `x_inp` provided by the `graph_model` and outputs being the predictions from the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_inp, outputs=prediction)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.005),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[metrics.categorical_accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, keeping track of its loss and accuracy on the training set, and its generalisation performance on the test set (we need to create another generator over the test data for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generator.flow(val_data.index, val_targets)\n",
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_gen,\n",
    "    verbose=0,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(history):\n",
    "    metrics = sorted(history.history.keys())\n",
    "    metrics = metrics[:len(metrics)//2]\n",
    "    for m in metrics:\n",
    "        # summarize history for metric m\n",
    "        plt.plot(history.history[m])\n",
    "        plt.plot(history.history['val_' + m])\n",
    "        plt.title(m)\n",
    "        plt.ylabel(m)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the model we can evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine if the classifier produces well-calibrated probabilities. Calibration curves also known as reliability diagrams are a visual method for this task. See reference [2] for a description of calibration curves also known as reliability diagrams.\n",
    "\n",
    "Diagnosis of model miscalibration should be performed on a held-out dataset that was not used for training. We are going to utilise our test set for this purpose. Equivalently, we can use our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes = test_data.index\n",
    "test_node_generator = generator.flow(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions holds the model's probabilistic output predictions\n",
    "test_predictions = predict(model, test_node_generator, n_predictions=n_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces a list of dictionaries (one entry per point in test set) and one entry\n",
    "# in the dictionary for each class label, label=1, label=2, label=3.\n",
    "y_test_pred = target_encoding.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a dataframe so that it is easier to work with the data\n",
    "test_pred_results = pd.DataFrame(y_test_pred)\n",
    "test_pred_results.index = test_data.index\n",
    "test_pred_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to draw one calibration curve for each column in `test_pred_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred_results.values\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data = []\n",
    "for i in range(test_pred.shape[1]):  # iterate over classes\n",
    "    calibration_data.append(calibration_curve(y_prob=test_pred[:, i], \n",
    "                                               y_true=test_targets[:, i], \n",
    "                                               n_bins=10, \n",
    "                                               normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data[0], type(calibration_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate Expected Calibration Error (ECE) for each class. See reference [2] for the definition of ECE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece = []\n",
    "for i in range(test_pred.shape[1]):\n",
    "    fraction_of_positives, mean_predicted_value = calibration_data[i]\n",
    "    ece.append(expected_calibration_error(prediction_probabilities=test_pred[:, i], \n",
    "                                          accuracy=fraction_of_positives, \n",
    "                                          confidence=mean_predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the reliability diagrams for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(calibration_data, test_pred, ece=ece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature scaling calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature scaling is an extension of [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling) for calibrating multi-class classification models. It was proposed in reference [2]. \n",
    "\n",
    "Temperature scaling uses a single parameter called the `temperature` to scale a classifier's non-probabilistic outputs (logits) before the application of the softmax operator that generates the model's probabilistic outputs.\n",
    "\n",
    "$\\hat{q}_i = \\max\\limits_{k} \\sigma_{SM}(\\mathbf{z}_i/T)^{(k)}$ \n",
    "\n",
    "where $\\hat{q}_i$ is the calibrated probability for the predicted class of the i-th node; $\\mathbf{z}_i$ is the vector of logits; $T$ is the temperature; $k$ is the k-th class; and, $\\sigma_{SM}$ is the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model gives the model's non-probabilistic outputs required for Temperature scaling.\n",
    "score_model = Model(inputs=x_inp, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training data such that inputs are the model output logits and corresponding true class labels are the one-hot encoded.\n",
    "\n",
    "We are going to train the calibration model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nodes = val_data.index\n",
    "val_node_generator = generator.flow(val_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_predictions = predict(score_model, test_node_generator, n_predictions=n_predictions)\n",
    "val_score_predictions = predict(score_model, val_node_generator, n_predictions=n_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_predictions.shape, val_score_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cal_train_all = val_score_predictions\n",
    "y_cal_train_all = val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the above data to a training and validation set. We are going to use the former for training the calibration model and the latter for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cal_train, x_cal_val, y_cal_train, y_cal_val = model_selection.train_test_split(x_cal_train_all, y_cal_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cal_train.shape, x_cal_val.shape, y_cal_train.shape, y_cal_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the calibration object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_model_temperature = TemperatureCalibration(epochs=1000)\n",
    "calibration_model_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the `fit` method to train the calibration model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_model_temperature.fit(x_train=x_cal_train, \n",
    "                                  y_train=y_cal_train, \n",
    "                                  x_val=x_cal_val, \n",
    "                                  y_val=y_cal_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_model_temperature.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the GraphSAGE logits, scale them by `temperature` and then apply the `softmax` to obtain the calibrated probabilities for each class.\n",
    "\n",
    "**Note** that scaling the logits by `temperature` does not change the predictions so the model's accuracy will not change and there is no need to recalculate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_calibrated_temperature = calibration_model_temperature.predict(x=test_score_predictions)\n",
    "test_predictions_calibrated_temperature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the calibration curves and calculate the ECE for each class. We should expect the ECE to be lower after calibration. If not, then a different calibration method should be considered, e.g., Isotonic Regression as described later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_after_temperature_scaling = []\n",
    "for i in range(test_predictions_calibrated_temperature.shape[1]):  # iterate over classes\n",
    "    calibration_data_after_temperature_scaling.append(calibration_curve(y_prob=test_predictions_calibrated_temperature[:, i], \n",
    "                                                             y_true=test_targets[:, i], \n",
    "                                                             n_bins=10, \n",
    "                                                             normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_after_scaling_temperature = []\n",
    "for i in range(test_predictions_calibrated_temperature.shape[1]):\n",
    "    fraction_of_positives, mean_predicted_value = calibration_data_after_temperature_scaling[i]\n",
    "    ece_after_scaling_temperature.append(expected_calibration_error(prediction_probabilities=test_predictions_calibrated_temperature[:, i], \n",
    "                                                        accuracy=fraction_of_positives, \n",
    "                                                        confidence=mean_predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_after_scaling_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(calibration_data_after_temperature_scaling, \n",
    "                    test_predictions_calibrated_temperature, \n",
    "                    ece=ece_after_scaling_temperature )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotonic Regression\n",
    "\n",
    "We extend [Isotonic calibration](https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html#sklearn.isotonic.IsotonicRegression) to the multi-class case by calibrating OVR models, one for each class. \n",
    "\n",
    "At test time, we calibrate the predictions for each class and then normalize the vector to unit norm so that the output of the calibration is a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the input to the Isotonic Calibration model is the classifier's probabilistic outputs as compared to Temperature scaling where the input was the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.shape # Holds the probabilistic predictions for each query node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probabilistic predictions for the validation set\n",
    "val_predictions = predict(model, val_node_generator, n_predictions=n_predictions)\n",
    "val_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the calibration object of type `IsotonicCalibration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_calib = IsotonicCalibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the `fit` method to train the calibraiton model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_calib.fit(x_train=val_predictions, y_train=val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_calibrated_isotonic = isotonic_calib.predict(test_pred)\n",
    "test_pred_calibrated_isotonic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the calibration curves and calculate the ECE for each class. We should expect the ECE to be lower after calibration. If not, then a different calibration method should be considered, e.g., Temperature Scaling as described earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_after_isotonic_scaling = []\n",
    "for i in range(test_pred_calibrated_isotonic.shape[1]):  # iterate over classes\n",
    "    calibration_data_after_isotonic_scaling.append(calibration_curve(y_prob=test_pred_calibrated_isotonic[:, i], \n",
    "                                                                      y_true=test_targets[:, i], \n",
    "                                                                      n_bins=10, \n",
    "                                                                      normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_after_scaling_isotonic = []\n",
    "for i in range(test_pred_calibrated_isotonic.shape[1]):\n",
    "    fraction_of_positives, mean_predicted_value = calibration_data_after_isotonic_scaling[i]\n",
    "    ece_after_scaling_isotonic.append(expected_calibration_error(prediction_probabilities=test_pred_calibrated_isotonic[:, i], \n",
    "                                                                 accuracy=fraction_of_positives, \n",
    "                                                                 confidence=mean_predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_after_scaling_isotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(calibration_data_after_isotonic_scaling,\n",
    "                    test_pred_calibrated_isotonic, \n",
    "                    ece=ece_after_scaling_isotonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ECE before and after calibration.\n",
    "\n",
    "Let's print the ECE for the original model before calibration and for the model after calibration using Temperature Scaling and Isotonic Regression.\n",
    "\n",
    "If model calibration is successful then either one or both of the calibrated models should have reduced ECE across all or most of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_error = \",\".join(format(e, \" 0.4f\") for e in ece)\n",
    "print(\"ECE before calibration:         {}\".format(cal_error))\n",
    "cal_error = \",\".join(format(e, \" 0.4f\") for e in ece_after_scaling_temperature)\n",
    "print(\"ECE after Temperature Scaling:  {}\".format(cal_error))\n",
    "cal_error = \",\".join(format(e, \" 0.4f\") for e in ece_after_scaling_isotonic)\n",
    "print(\"ECE after Isotonic Calibration: {}\".format(cal_error) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalculate classifier accuracy before and after calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(test_pred, axis=1)\n",
    "y_pred_calibrated_temperature = np.argmax(test_predictions_calibrated_temperature, axis=1)\n",
    "y_pred_calibrated_isotonic = np.argmax(test_pred_calibrated_isotonic, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accurace before calibration:         {:.2f}\".format(accuracy_score(y_pred=y_pred, \n",
    "                                                                          y_true=np.argmax(test_targets, axis=1))))\n",
    "print(\"Accurace after Temperature Scaling:  {:.2f}\".format(accuracy_score(y_pred=y_pred_calibrated_temperature, \n",
    "                                                                          y_true=np.argmax(test_targets, axis=1))))\n",
    "print(\"Accurace after Isotonic Calibration: {:.2f}\".format(accuracy_score(y_pred=y_pred_calibrated_isotonic, \n",
    "                                                                          y_true=np.argmax(test_targets, axis=1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use temperature scaling and isotonic regression to calibrate the output probabilities of a GraphSAGE model used for multi-class node attribute inference."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
